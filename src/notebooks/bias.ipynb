{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_policy(reward_matrix):\n",
    "\n",
    "    policy = (reward_matrix == np.max(reward_matrix, axis=1)[:, None]).astype(float)\n",
    "\n",
    "    return policy / np.sum(policy, axis=1)[:, None]\n",
    "\n",
    "\n",
    "def get_perceived_score(logged_reward_matrix, policy, n_interactions):\n",
    "\n",
    "    return np.sum(policy * logged_reward_matrix) / n_interactions\n",
    "\n",
    "\n",
    "def get_logged_reward(reward_matrix, cust_segments, policy, n_interactions):\n",
    "\n",
    "    assignment_matrix = n_interactions * cust_segments[:, None] * policy\n",
    "    logged_reward_matrix = assignment_matrix * reward_matrix\n",
    "\n",
    "    return  logged_reward_matrix, assignment_matrix\n",
    "\n",
    "\n",
    "def get_true_score(reward_matrix, cust_segments, policy):\n",
    "\n",
    "    logged_reward_matrix, _ = get_logged_reward(reward_matrix, cust_segments, policy, 1)\n",
    "\n",
    "    return np.round(np.sum(logged_reward_matrix), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_x_a = np.array([\n",
    "    [.4, .6],\n",
    "    [.7, .5]\n",
    "])\n",
    "\n",
    "p_x = np.array([.4 , .6])\n",
    "\n",
    "policy_0 = np.array([\n",
    "    [.7, .3],\n",
    "    [.2, .8]\n",
    "])\n",
    "\n",
    "n_interactions = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_x, n_a = q_x_a.shape\n",
    "logged_reward_matrix_0, assignment_matrix_0 = get_logged_reward(q_x_a, p_x, policy_0, n_interactions)\n",
    "\n",
    "records_0 = {'x': [], 'a': [], 'r': []}\n",
    "for x in range(n_x):\n",
    "    for a in range(n_a):\n",
    "        records_0['x'] += [x] * int(assignment_matrix_0[x, a])\n",
    "        records_0['a'] += [a] * int(assignment_matrix_0[x, a])\n",
    "        records_0['r'] += [1] * int(logged_reward_matrix_0[x, a])\n",
    "        records_0['r'] += [0] * int(assignment_matrix_0[x, a] - logged_reward_matrix_0[x, a])\n",
    "\n",
    "records_0 = pd.DataFrame(records_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[280., 120.],\n",
       "       [120., 480.]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assignment_matrix_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logged_reward_matrix for policy_0\n",
      "[[112.  72.]\n",
      " [ 84. 240.]]\n"
     ]
    }
   ],
   "source": [
    "print('logged_reward_matrix for policy_0')\n",
    "print(logged_reward_matrix_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_trueoptimal = get_best_policy(q_x_a)\n",
    "policy_percieveoptimal = get_best_policy(logged_reward_matrix_0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true optimal policy\n",
      "[[0. 1.]\n",
      " [1. 0.]]\n",
      "percieved optimal policy\n",
      "[[1. 0.]\n",
      " [0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "print('true optimal policy')\n",
    "print(policy_trueoptimal)\n",
    "print('percieved optimal policy')\n",
    "print(policy_percieveoptimal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true optimal policy true score: 0.66\n",
      "percieved optimal policy true score: 0.46\n",
      "policy_0 true score: 0.508\n"
     ]
    }
   ],
   "source": [
    "print('true optimal policy true score:', get_true_score(q_x_a, p_x, policy_trueoptimal))\n",
    "print('percieved optimal policy true score:', get_true_score(q_x_a, p_x, policy_percieveoptimal))\n",
    "print('policy_0 true score:', get_true_score(q_x_a, p_x, policy_0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true optimal policy score bias: 0.156\n",
      "percieved optimal policy score bias: 0.352\n",
      "policy_0 score bias: 0.30879999999999996\n"
     ]
    }
   ],
   "source": [
    "print('true optimal policy score bias:', get_perceived_score(logged_reward_matrix_0, policy_trueoptimal, n_interactions))\n",
    "print('percieved optimal policy score bias:', get_perceived_score(logged_reward_matrix_0, policy_percieveoptimal, n_interactions))\n",
    "print('policy_0 score bias:', get_perceived_score(logged_reward_matrix_0, policy_0, n_interactions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logged reward generated under optimal policy\n",
      "[[  0. 240.]\n",
      " [420.   0.]]\n",
      "evaluated reward of optimal policy using original logged reward with propensity score adjustment\n",
      "[[  0. 240.]\n",
      " [420.   0.]]\n"
     ]
    }
   ],
   "source": [
    "print('logged reward generated under optimal policy')\n",
    "print(get_logged_reward(q_x_a, p_x, policy_trueoptimal, n_interactions)[0])\n",
    "print('evaluated reward of optimal policy using original logged reward with propensity score adjustment')\n",
    "print(policy_trueoptimal * logged_reward_matrix_0 / policy_0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adjusting the evaluated reward by propensity score will transform the distribution of loggged reward as if it was generated by the evaluated policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logged_reward_matrix_0 / policy_0\n",
      "[[160. 240.]\n",
      " [420. 300.]]\n",
      "n_interactions * p_x * q_x_a\n",
      "[[160. 240.]\n",
      " [420. 300.]]\n"
     ]
    }
   ],
   "source": [
    "print('logged_reward_matrix_0 / policy_0')\n",
    "print(logged_reward_matrix_0 / policy_0)\n",
    "print('n_interactions * p_x * q_x_a')\n",
    "print(n_interactions * p_x[:, None] * q_x_a)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adjust logged_reward_matrix_0 by propensity score leads to the reward matrix that looks like it wasn't subjected to any policy (i.e. all actions were played at the same time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ope",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
